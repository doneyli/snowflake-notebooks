{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Synthetic Questions and Answers for RAG Evaluation\n",
    "\n",
    "This notebook demonstrates how to use Cortex to generate synthetic questions and answers for evaluating a RAG application\n",
    "\n",
    "This is a following the example in the [Hugging Face RAG evaluation notebook](https://huggingface.co/learn/cookbook/rag_evaluation), but using Cortex to generate the synthetic questions and answers and performing the operations in Snowpark (and not on the local machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowpark for Python\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.types import Variant\n",
    "from snowflake.snowpark.version import VERSION\n",
    "\n",
    "# Snowpark ML\n",
    "# Misc\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging \n",
    "logger = logging.getLogger(\"snowflake.snowpark.session\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "from snowflake import connector\n",
    "from snowflake.ml.utils import connection_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../creds.json') as f:\n",
    "    data = json.load(f)\n",
    "    USERNAME = data['user']\n",
    "    PASSWORD = data['password']\n",
    "    SF_ACCOUNT = data['account']\n",
    "    SF_WH = data['warehouse']\n",
    "\n",
    "CONNECTION_PARAMETERS = {\n",
    "   \"account\": SF_ACCOUNT,\n",
    "   \"user\": USERNAME,\n",
    "   \"password\": PASSWORD,\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(CONNECTION_PARAMETERS).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowflake_environment = session.sql('select current_user(), current_version()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "from snowflake.ml import version\n",
    "mlversion = version.VERSION\n",
    "\n",
    "\n",
    "# Current Environment Details\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(session.get_current_role()))\n",
    "print('Database                    : {}'.format(session.get_current_database()))\n",
    "print('Schema                      : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse                   : {}'.format(session.get_current_warehouse()))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][1]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))\n",
    "print('Snowflake ML version        : {}.{}.{}'.format(mlversion[0],mlversion[2],mlversion[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "I start with the SEC RAG example where the data has already been chunked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import col\n",
    "import snowflake.snowpark.functions as f\n",
    "from snowflake.snowpark.functions import col, split, lit, regexp_extract\n",
    "from snowflake.cortex import Complete\n",
    "\n",
    "\n",
    "article_df = session.table(\"RAJIV.PUBLIC.CONTENT_CHUNKS_10K\")\n",
    "filtered_df = article_df.filter(col(\"company_name\").like(\"RIVIAN%\"))\n",
    "\n",
    "# Now you can perform further operations on filtered_df, like displaying the data\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will only want to select a subset of the chunks to build your generated examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df = filtered_df.sample(n=50)\n",
    "shuffled_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the Q&A pairs for the RAG evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf = shuffled_df.withColumn(\n",
    "    \"GenQA\",\n",
    "    Complete(\n",
    "        model='mixtral-8x7b',prompt = f.concat(\n",
    "            f.lit(QA_generation_prompt),\n",
    "            f.col(\"CONTENT_CHUNK\"),\n",
    "            f.lit(\"\\n Output:::\")\n",
    "            ))\n",
    ")\n",
    "outputs = outdf.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.GENQA[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf = outdf.withColumn(\"split_col\", split(col(\"GENQA\"), lit('\\n')))\n",
    "\n",
    "# Create new columns 'GENQ' and 'GENA' by accessing elements of the array\n",
    "outdf = outdf.withColumn(\"GENQ\", col(\"split_col\")[0])\n",
    "outdf = outdf.withColumn(\"GENA\", col(\"split_col\")[1])\n",
    "\n",
    "outdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save our work periodically\n",
    "table_name = \"RAJIV.PUBLIC.CONTENT_CHUNKS_10K_GQA\"  # Specify your new table name here\n",
    "outdf.write.mode(\"overwrite\").save_as_table(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf = session.table(\"RAJIV.PUBLIC.CONTENT_CHUNKS_10K_GQA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critique\n",
    "\n",
    "The questions generated by the previous agent can have many flaws: we should do a quality check before validating these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groundness Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a integer between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf = outdf.withColumn(\n",
    "    \"Eval_Groundness\",\n",
    "    Complete(\n",
    "        model='mixtral-8x7b',prompt = f.concat(\n",
    "            f.lit(question_groundedness_critique_prompt),\n",
    "            f.col(\"GENQ\"),\n",
    "            f.lit(\"\\n Context: \"),\n",
    "            f.col(\"CONTENT_CHUNK\"),\n",
    "            f.lit(\"\\n Answer:::\")\n",
    "            ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## you can always pull this into pandas to see the results\n",
    "outputs = outdf.to_pandas()\n",
    "outputs.EVAL_GROUNDNESS[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split out the score,which will later filter on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the score by taking the rightmost character of the last part of split_rating\n",
    "outdf = outdf.withColumn(\"score_EG\", regexp_extract(col(\"EVAL_GROUNDNESS\"), r\"Total rating:\\s*(\\d)\", 1).cast(\"int\"))\n",
    "\n",
    "\n",
    "# Show results to verify\n",
    "outdf.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_table_name = \"RAJIV.PUBLIC.CONTENT_CHUNKS_10K_GQA_All\"  # Specify your new table name here\n",
    "# Write the DataFrame to a new table in Snowflake\n",
    "outdf.write.mode(\"overwrite\").save_as_table(working_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to analysts.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as an integer between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf = outdf.withColumn(\n",
    "    \"EVAL_RELEVANCE\",\n",
    "    Complete(\n",
    "        model='mixtral-8x7b',prompt = f.concat(\n",
    "            f.lit(question_relevance_critique_prompt),\n",
    "            f.col(\"GENQ\"),\n",
    "            f.lit(\"\\n Answer:::\")\n",
    "            ))\n",
    ")\n",
    "\n",
    "# Extracting the score by taking the rightmost character of the last part of split_rating\n",
    "outdf = outdf.withColumn(\"score_ER\", regexp_extract(col(\"EVAL_RELEVANCE\"), r\"Total rating:\\s*(\\d)\", 1).cast(\"int\"))\n",
    "\n",
    "outdf.write.mode(\"overwrite\").save_as_table(working_table_name)\n",
    "\n",
    "# Show results to verify\n",
    "outdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critique Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf = session.table(working_table_name)\n",
    "outdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as an integer between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf = outdf.withColumn(\n",
    "    \"EVAL_CRITIQUE\",\n",
    "    Complete(\n",
    "        model='mixtral-8x7b',prompt = f.concat(\n",
    "            f.lit(question_relevance_critique_prompt),\n",
    "            f.col(\"GENQ\"),\n",
    "            f.lit(\"\\n Answer:::\")\n",
    "            ))\n",
    ")\n",
    "\n",
    "# Extracting the score by taking the rightmost character of the last part of split_rating\n",
    "outdf = outdf.withColumn(\"score_EC\", regexp_extract(col(\"EVAL_CRITIQUE\"), r\"Total rating:\\s*(\\d)\", 1).cast(\"int\"))\n",
    "\n",
    "outdf.write.mode(\"overwrite\").save_as_table(working_table_name)\n",
    "\n",
    "# Show results to verify\n",
    "outdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep only the questions that have scored well, by filtering on the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = outdf.filter((outdf['SCORE_EC'] > 3) & (outdf['SCORE_ER'] > 3) & (outdf['SCORE_EG'] > 3))\n",
    "filtered_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df = filtered_df.select(\"CONTENT_CHUNK\", \"GENQ\", \"GENA\")\n",
    "# Now, to save this DataFrame as a new table in Snowflake\n",
    "selected_df.write.mode(\"overwrite\").saveAsTable(\"RAJIV.PUBLIC.CONTENT_CHUNKS_10K_GQA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_public",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
